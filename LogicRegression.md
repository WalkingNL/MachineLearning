在本文中，准备介绍一下**逻辑回归**，主要是学习的内容，谈不上有多么深入。这短短的几年工作中，主要做开发，平时真的很少再看机器学习相关的内容，最近有空，看了一些，所以做个简单的输出。

## 逻辑回归
在真正介绍逻辑回归之前，先看一下竞价广告这个经典的案例。竞价广告作为商用搜索引擎公司的主要收入来源，广告商提供创意，并对关键词竞价，当用户在搜索引擎中，搜索某个关键词时，就会触发那些购买了这个关键词的广告商，对于这些被触发的广告商的广告，就存在一个创意排名的问题，一般按照`eCPM`对这些广告进行排名，公式如下：

    eCPM = CTR * BidPrice
  * `eCPM`代表每千次展示可获取的收入；
  * `BidPrice`代表广告商对竞价关键词的出价，出价越高，排名越靠前；
  * `CTR`表示对广告的点击率，通常情况下，是指广告的点击率与展示次数的比值。反映出广告与用户需求的匹配程度，`CTR`越高，广告排名也就越高。

能够很明确的看出，`eCPM`与`CTR`和`BidPrice`这两个变量正相关。在兼顾用户体验和广告商排名需求的情况下，搜索引擎公司通过追求最佳eCPM，来获取最大化的收益。其中，对于`BidPrice`是先前就确定好了的，这样一来，最终决定`eCPM`大小的就只有`CTR`了。所以说，要在广告投放前，预知`eCPM`的大小，实际就是预测`CTR`。那么如何预测呢？

存在两种思路：
1. 根据历史点击率进行估算。
2. 采用逻辑回归等机器学习模型进行估算。

对于第一种思路，根据历史点击率进行估算，这是最为直观的一种做法。比如针对广告`A`的某个关键词`W`，如果广告`A`在1000次的展示中，有10次被点击，那么`CTR = 0.01`。首先，可以看到这种思路是正向的，也就像我们通常说的**后验概率**。就如同一个袋子里面有除颜色之外，其它属性完全一样的两种球，我们知道了球的总数，也分别知道每一种球的个数，现在要计算出从袋子中随机的取出一种球的概率。问题就在于，如果不知道这些信息————球的总数，以及每一种球的个数，那该如何计算每一种球的概率呢？此外，即使有历史信息，但也很有可能存在历史信息稀少或是稀疏的情况。我们假设估算`CTR`就是针对`<关键词，广告>`这样的数据对的，如果有1000个广告，对应着1000个关键词，意味着有1000000个`CTR`要进行估算，真实情况下，数据往往难达到这种要求。更何况这里只假设有1000个广告，实际情况会更多。针对这些问题，就需要另觅其它技术手段估算广告的`CTR`，那么逻辑回归作为一种很好的预估`CTR`的解决办法出现了。

### 逻辑回归的原理
首先二项逻辑回归是一种用以分类的模型，使用条件概率分布`P(y|X)`表示，其中的随机变量*X*的定义域为**R**，随机变量*y*的取值为-1或1。采用的公式如下：

![](https://github.com/WalkingNL/Pics/blob/master/rule1.jpg)

其中，*X*是输入变量，*y*是输出，代表不同的类别，*W*是特征权重向量。适配到上面的案例中，也就是说这个公式对于估算上面的`CTR`而言，首先我们用`y = 1`表示用户点击广告，相反`y = -1`表示用户没有点击广告。变量*X*表示<关键词，广告>这样的数据对构成的特征向量，*W*是这些广告的权重向量，而权重大小，一般体现的是对应特征的重要程度。现在回到上面的公式中，我们看到一旦权重向量*W*是已知的，那么只要输入*X*就能预估出`CTR`。

现在的问题是，如何知道*W*呢？所以问题就转换成根据训练数据求特征权重。这个时候，就需要把历史的点击数据<查询，广告>作为训练集合，使用最大似然法对模型参数进行估计，公式如下面所示：

![](https://github.com/WalkingNL/Pics/blob/master/rule3.jpg)

其中，我们假设共有n个训练数据,(*Xi*, *yi*)作为当中的第*i*个训练实例，分正负两类样本，现在对上面公式(3)取负log，就成下面的公式(4)，如下示。此公式中，为了避免过拟合的问题，还加入了L1正则化项，即λ||*W*||部分。但是否一定要加这部分，我们稍后讨论。

![](https://github.com/WalkingNL/Pics/blob/master/rule4.jpg)

现在看原来的问题就变成了现在的以对数似然函数为目标函数的**无约束最优化**问题。解决这类问题，一般可以采用类似梯度下降法、牛顿法或者拟牛顿法。遵循的通用的流程是：(1) 对初始的权重向量*W0*赋予0附近的随机值；(2) 然后是反复迭代，在每一次迭代的过程中，依据当前的权重向量*W*计算目标函数的最快下降方向，并更新权重向量`W(t) = W(t-1) + aDt`; (3) 最后，当目标函数稳定到了某个极值点，此时的权重向量就作为目标函数的最优解。
