在本文中，准备介绍一下**逻辑回归**，主要是学习的内容，谈不上有多么深入。这短短的几年工作中，主要做开发，平时真的很少再看机器学习相关的内容，最近有空，看了一些，所以做个简单的输出。

## 逻辑回归
在真正介绍逻辑回归之前，先看一下竞价广告这个经典的案例。竞价广告作为商用搜索引擎公司的主要收入来源，广告商提供创意，并对关键词竞价，当用户在搜索引擎中，搜索某个关键词时，就会触发那些购买了这个关键词的广告商，对于这些被触发的广告商的广告，就存在一个创意排名的问题，一般按照`eCPM`对这些广告进行排名，公式如下：

    eCPM = CTR * BidPrice
  * `eCPM`代表每千次展示可获取的收入；
  * `BidPrice`代表广告商对竞价关键词的出价，出价越高，排名越靠前；
  * `CTR`表示对广告的点击率，通常情况下，是指广告的点击率与展示次数的比值。反映出广告与用户需求的匹配程度，`CTR`越高，广告排名也就越高。

能够很明确的看出，`eCPM`与`CTR`和`BidPrice`这两个变量正相关。在兼顾用户体验和广告商排名需求的情况下，搜索引擎公司通过追求最佳eCPM，来获取最大化的收益。其中，对于`BidPrice`是先前就确定好了的，这样一来，最终决定`eCPM`大小的就只有`CTR`了。所以说，要在广告投放前，预知`eCPM`的大小，实际就是预测`CTR`。那么如何预测呢？

存在两种思路：
1. 根据历史点击率进行估算。
2. 采用逻辑回归等机器学习模型进行估算。

对于第一种思路，根据历史点击率进行估算，这是最为直观的一种做法。比如针对广告`A`的某个关键词`W`，如果广告`A`在1000次的展示中，有10次被点击，那么`CTR = 0.01`。首先，可以看到这种思路是正向的，也就像我们通常说的**后验概率**。就如同一个袋子里面有除颜色之外，其它属性完全一样的两种球，我们知道了球的总数，也分别知道每一种球的个数，现在要计算出从袋子中随机的取出一种球的概率。问题就在于，如果不知道这些信息————球的总数，以及每一种球的个数，那该如何计算每一种球的概率呢？此外，即使有历史信息，但也很有可能存在历史信息稀少或是稀疏的情况。我们假设估算`CTR`就是针对`<关键词，广告>`这样的数据对的，如果有1000个广告，对应着1000个关键词，意味着有1000000个`CTR`要进行估算，真实情况下，数据往往难达到这种要求。更何况这里只假设有1000个广告，实际情况会更多。针对这些问题，就需要另觅其它技术手段估算广告的`CTR`，那么逻辑回归作为一种很好的预估`CTR`的解决办法出现了。

### 逻辑回归的原理
首先二项逻辑回归是一种用以分类的模型，使用条件概率分布`P(y|X)`表示，其中的随机变量*X*的定义域为**R**，随机变量*y*的取值为-1或1。采用的公式如下：

![](https://github.com/WalkingNL/Pics/blob/master/rule1.jpg)

其中，*X*是输入变量，*y*是输出，代表不同的类别，*W*是特征权重向量。适配到上面的案例中，也就是说这个公式对于估算上面的`CTR`而言，首先我们用`y = 1`表示用户点击广告，相反`y = -1`表示用户没有点击广告。变量*X*表示<关键词，广告>这样的数据对构成的特征向量，*W*是这些广告的权重向量，而权重大小，一般体现的是对应特征的重要程度。现在回到上面的公式中，我们看到一旦权重向量*W*是已知的，那么只要输入*X*就能预估出`CTR`。

现在的问题是，如何知道*W*呢？所以问题就转换成根据训练数据求特征权重。这个时候，就需要把历史的点击数据<查询，广告>作为训练集合，使用最大似然法对模型参数进行估计，公式如下面所示：

![](https://github.com/WalkingNL/Pics/blob/master/rule3.jpg)

其中，我们假设共有n个训练数据,(*Xi*, *yi*)作为当中的第*i*个训练实例，分正负两类样本，现在对上面公式(3)取负log，就成下面的公式(4)，如下示。此公式中，为了避免过拟合的问题，还加入了L1正则化项，即λ||*W*||部分。但是否一定要加这部分，我们稍后讨论。

![](https://github.com/WalkingNL/Pics/blob/master/rule4.jpg)

现在看原来的问题就变成了现在的以对数似然函数为目标函数的**无约束最优化**问题。解决这类问题，一般可以采用类似梯度下降法、牛顿法或者拟牛顿法。遵循的通用的流程是：(1) 对初始的权重向量*W0*赋予0附近的随机值；(2) 然后是反复迭代，在每一次迭代的过程中，依据当前的权重向量*W*计算目标函数的最快下降方向，并更新权重向量`W(t) = W(t-1) + aDt`; (3) 最后，当目标函数稳定到了某个极值点,结束迭代，此时的权重向量W(t)就作为目标函数的最优解。

对于梯度下降法，牛顿法及拟牛顿法，这几个算法的主要区别在于目标函数下降方向*D*(*t*)的计算方式是不同的，*D*(*t*)是通过对目标函数权重向量*W*当前取值下求导数(梯度)或者求二阶导数(海森矩阵)来确定的。

梯度下降法采用目标函数把当前权重向量*W*的梯度的反方向作为下降方向：

    D(t) = -G(t) = -∇wf(W(t))   公式(5)
其中G(t)是目标函数的梯度，计算方法如下：

![](https://github.com/WalkingNL/Pics/blob/master/rule5.jpg)

梯度下降法针对求解无约束优化问题最为常用的方法，但它的有点与它的缺点都很突出
  * 优点，实现上相对简单。
  * 缺点，收敛较慢，如果不是非凸函数的话，极值点不能保证就是最小值
  
相比之下牛顿法在实现上，就要复杂一下，需要在当前的权重*W*之下，利用二阶泰勒展开函数作为近似目标函数，然后再对这个近似的目标函数求解下降方向。公式如下：
    
    D(t) = -B(t)^(-1)∇wf(W(t))   公式(7)
其中，B(t)是目标函数在*Wt*处的海森矩阵，这个搜索方向被称为牛顿方向，它克服了梯度下降法收敛速度慢的不足，但从公式中也能看出，这里还需要计算逆矩阵，所以实现难度会比梯度下降法大许多。

为此，拟牛顿法的出现，在难度上，就要缓和许多。不用直接计算海森矩阵及其逆矩阵，只需分摊到每一步当中计算目标函数梯度，然后通过正定矩阵来拟合海森矩阵的逆矩阵。这也能够看出，它和牛顿法的区别并没有方法上的差异，只在体现在过程上，做了一些改进而已。那么在实际中，也有许多的采用拟牛顿法的方法，比如[BFGS](https://rtmath.net/help/html/9ba786fe-9b40-47fb-a64c-c3a492812581.htm)和[L-BFGS](http://aria42.com/blog/2014/12/understanding-lbfgs)，后者是对前者的改进。具体可以点到链接里面看看，其中对于`BFGS`的介绍，还是比较浅显易懂；而后一个链接对L-BFGS的介绍，是对比着BFGS进行的，如果有一定的基础，看起来并不会费劲儿。


## 后记
今天先介绍到这里，不得不说，这个写起来难度要大很多，主要还是因为自己的基础储备不够，后面再更新吧。我需要找专门的人交流一下，因为就单纯看写的这点内容来看，我就觉得很生硬，所以这肯定是不行的。不管咋样，还是很高兴，能开始这样的一个主题，也希望后面能尽快的补充更多的内容。


关于在公式(4)中，加入的L1正则化项，目的是为了避免过拟合。其实很显然，L1正则化项的加入并非一定需要。因为如果不加入它，利用的训练数据训练时，并没有出现过拟合的问题，那加入它势必会出另一个问题，即欠拟合。因为我没有这方面的经验，所以解答不了这个问题，就找了一个同学问了一下，什么时候才应该加入L1正则化项。对方的回答和我的猜测倒是很接近，就是如果用样本数据训练的时候，出现了过拟合，就应该考虑加入L1正则化项了。但这也引出了另一个思考，就是不加出现过拟合的概率大呢还是加上出现欠拟合的概率大。如果是前者，就应该先加上，如果真出现了欠拟合，再去掉不就行了。那么问题又来了，怎么才能知道哪种情况的概率大呢？这或许要根据经验来决策吧，这个问题先放这里吧。不过老实说，我同学给的答案我并不认可，因为模型训练需要一个很长的过程，如果真等到过拟合问题出现了，再加上L1正则化项来避免，是不是有点晚了。而且，L1正则化项就一定能避免过拟合吗？对于这个问题，我同学给出了这么一个解释，有点文邹邹的，我不太懂，原话是：**当加上L1正则化项时，在误差反向传播过程中，参数的更新会受到约束**。很明显，这个解释说服力不强的，因为这个解释并没有一个强点支撑住L1加入的必然效果。（Oct 27，2019）
